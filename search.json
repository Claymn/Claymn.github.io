[{"title":"Spark源码编译","url":"/2019/10/13/Spark-compile/","content":"Spark源码编译环境：Mac OS 10.15；jdk 1.8；Spark-2.3.0；Maven-3.3.9。准备好环境以后，在Spark-2.3.0文件目录下运行\n./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0\n由于使用的是cdh版本的hadoop2.6.0，导致在编译时会出现错误：\n[ERROR] Failed to execute goal on project spark-launcher_2.11: Could not resolve dependencies for project org.apache.spark:spark-launcher_2.11:jar:2.3.0: Failure to find org.apache.hadoop:hadoop-client:jar:2.6.0-cdh5.7.0 in https://repo1.maven.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR]   mvn &lt;goals&gt; -rf :spark-launcher_2.11\n在编译时会找不到包而导致编译失败。因为Spark默认的数据源是Apache，而在编译时使用的是cdh版本的hadoop。所以要在pom.xml文件中修改数据源。\n&lt;repositories&gt;    &lt;repository&gt;      &lt;id&gt;central&lt;/id&gt;      &lt;!-- This should be at top, it makes maven try the central repo first and then others and hence faster dep resolution --&gt;      &lt;name&gt;Maven Repository&lt;/name&gt;      &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt;      &lt;releases&gt;        &lt;enabled&gt;true&lt;/enabled&gt;      &lt;/releases&gt;      &lt;snapshots&gt;        &lt;enabled&gt;false&lt;/enabled&gt;      &lt;/snapshots&gt;    &lt;/repository&gt;-----------------------------------------------------------------------------------------    &lt;repository&gt;      &lt;id&gt;cloudera&lt;/id&gt;      &lt;name&gt;cloudera Repository&lt;/name&gt;      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;   &lt;/repository&gt;-----------------------------------------------------------------------------------------  &lt;/repositories&gt;\n在Spark的pom.xml文件中添加两横线中间的内容，将cloudera数据源添加进去。之后再按照Spark官网给出的编译指南进行编译即可。\n","categories":["Spark"],"tags":["Spark"]},{"title":"在CentOS 7环境下编译Hadoop-2.7.7源码","url":"/2020/04/14/Hadoop-deploy/","content":"最近在学习Hadoop，所以下载了源码自己进行编译安装。虽然官网提供编译好的二进制包，但是由于系统版本的细微差别。所以决定自己动手编译，日后的工作中难免会要自定义源码中的组件，先学习如何编译源码，避免以后踩坑。\n环境准备系统环境：CentOS 7Hadoop版本：Hadoop-2.7.7\n编译所需要的组件先解压从官网下载的hadoop-2.7.7-src.tar.gz\ntar -zxvf hadoop-2.7.7-src.tar.gz -C /root/apps \n/root/apps可以换成其他想要的路径。然后进入到解压后的目录，查看BUILDING.txt文件编译Hadoop所需要的组件：\nBuild instructions for Hadoop----------------------------------------------------------------------------------Requirements:* Unix System* JDK 1.7+* Maven 3.0 or later* Findbugs 1.3.9 (if running findbugs)* ProtocolBuffer 2.5.0* CMake 2.6 or newer (if compiling native code), must be 3.0 or newer on Mac* Zlib devel (if compiling native code)* openssl devel ( if compiling native hadoop-pipes and to get the best HDFS encryption performance )* Linux FUSE (Filesystem in Userspace) version 2.6 or above ( if compiling fuse_dfs )* Internet connection for first build (to fetch all Maven and Hadoop dependencies)\n所以，我们需要在编译Hadoop之前先安装JDK、Maven、Ant、Findbugs、ProtocolBuffer、CMake、openssl、ncurses-devel。\n编译需要的包yum install cmakeyum install openssl-develyum install ncurses-devel\n\n编译需要的软件JDKLinux系统自带有java环境，但是并不是我们需要的。所以要先删除系统中自带的Java环境。\n// 卸载OpenJDKrpm -qa|grep javarpm -e --nodeps xxxxxxxxxx(OpenJDK的包)\n卸载掉OpenJDK以后再开始安装JDK：\n// 解压tar -zxvf jdk-8u231-linux-x64.tar.gz -C /root/apps// 将JDK的路径添加到环境变量中去vi ~/.bash_profileexport JAVA_HOME=/root/apps/jdk1.8.0_231export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tool.jarsource ~/.bash_profile//  检查安装是否成功java -version// 下面的结果就是已经安装成功java version &quot;1.8.0_231&quot;Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)\nMaven// 解压tar -zxvf apache-maven-3.3.9-bin.tar.gz -C /root/apps// 将maven路径添加到环境变量中vi ~/.bash_profileexport MAVEN_HOME=/root/apps/apache-maven-3.3.9export PATH=$MAVEN_HOME/bin:$PATHsource ~/.bash_profile// 检查是否安装成功mvn -vApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /root/apps/apache-maven-3.3.9Java version: 1.8.0_231, vendor: Oracle CorporationJava home: /root/apps/jdk1.8.0_231/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;3.10.0-1062.el7.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;\n\nAnt// 解压tar -zxvf apache-ant-1.9.14-bin.tar.gz -C /root/apps// 将路径添加到环境变量中vi ~/.bash_profileexport ANT_HOME=/root/apps/apache-ant-1.9.14export PATH=$ANT_HOME/bin:$PATHsource ~/.bash_profile// 检查是否安装成功ant -versionApache Ant(TM) version 1.9.14 compiled on March 12 2019\nFindbugs// 解压tar -zxvf findbugs-3.0.1.tar.gz -C /root/apps// 将路径添加到环境变量中vi ~/.bash_profileexport FIND_HOME=/root/apps/findbugs-3.0.1export PATH=$FIND_HOME/bin:$PATHsource ~/.bash_profile// 检查是否安装成功findbugs -version3.0.1\n\nProtocol Buffer// 安装C++依赖yum install gcc-c++// 解压tar -zxvf protobuf-2.5.0.tar.gz -C /root/apps// 编译后的文件如果需要放到其他文件目录mkdir /root/apps/protobuf// 在解压后的目录下配置编译好的软件安装目录./configure --prefix=/root/apps/protobuf// 编译make make install// 将路径添加到环境变量中vi ~/.bash_profileexport PROTO_HOME=/root/apps/protobuf-2.5export PATH=$PROTO_HOME/bin:$PATHsource ~/.bash_profile// 检查是否安装成功protoc --versionlibprotoc 2.5.0\n#编译Hadoop\n// 解压tar -zxvf hadoop-2.7.7-src.tar.gz -C /root/appscd hadoop-2.7.7-src// 开始编译mvn package -Pdisk,native -DskipTests -Dtar\n编译需要比较长的一段时间，编译好的.tar.gz存放在/root/apps/hadoop-2.7.7-src/hadoop-dist/target里。可以将编译好的安装包放到其他路径解压安装。解压安装之后也要将路径配置到环境变量中去，最后可以使用hadoop version命令来检查是否安装成功。\n","categories":["Hadoop"],"tags":["Hadoop"]}]